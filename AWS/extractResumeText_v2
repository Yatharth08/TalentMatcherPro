import time
import boto3
import json
import re
from opensearchpy import OpenSearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

# Initialize region
region = 'us-east-1'

# Initialize DynamoDB
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('ResumeMetadata')

# Initialize textract
textract = boto3.client('textract', region_name=region)

# Initialize textract
bedrock = boto3.client('bedrock-runtime')

# Initialize sqs
sqs = boto3.client('sqs', region_name=region)
QUEUE_URL = "https://sqs.us-east-1.amazonaws.com/058264455237/AmazonTextractQueue"

# Auth setup for OpenSearch
credentials = boto3.Session().get_credentials()
awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, 'es', session_token=credentials.token)

OPENSEARCH_HOST = 'search-job-resume-embeddings-2jgzspc23gnl5hy6z5vrulexgm.us-east-1.es.amazonaws.com'

INDEX_NAME = 'resumes'

opensearch = OpenSearch(
    hosts=[{'host': OPENSEARCH_HOST, 'port': 443}],
    http_auth=awsauth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection
)

def get_textract_results(job_id, max_retries=5, wait_interval=2):
    attempt = 0
    while attempt < max_retries:
        response = textract.get_document_text_detection(JobId=job_id)
        status = response.get("JobStatus")
        print(f"Attempt {attempt + 1} - Job status: {status}")

        if status == "SUCCEEDED":
            break
        elif status in ["FAILED", "PARTIAL_SUCCESS", "STOPPED"]:
            print(f"Textract job failed with status: {status}")
            return []
        else:
            time.sleep(wait_interval)
            attempt += 1
    else:
        print("Textract job not ready after max retries.")
        return []

    text_lines = []
    next_token = None
    while True:
        if next_token:
            response = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)
        else:
            response = textract.get_document_text_detection(JobId=job_id)

        for block in response.get("Blocks", []):
            if block.get("BlockType") == "LINE":
                text_lines.append(block.get("Text"))

        next_token = response.get("NextToken")
        if not next_token:
            break

    return text_lines

def get_completion(prompt):
    try:
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 4000,
            "temperature": 0.2
        }

        response = bedrock.invoke_model(
            modelId="anthropic.claude-3-sonnet-20240229-v1:0",
            contentType="application/json",
            accept="application/json",
            body=json.dumps(request_body)
        )

        decoded_body = response["body"].read().decode("utf-8")
        result = json.loads(decoded_body)

        extracted_content = ""
        if isinstance(result.get("content"), list):
            for content_block in result["content"]:
                if content_block.get("type") == "text":
                    extracted_content += content_block.get("text", "")

        return extracted_content

    except Exception as e:
        print("Error in Bedrock model invocation:", e)
        return ""

def get_entity(text):
    listofparameters = ['name','education', 'experience', 'skills', 'phone', 'address_location']
    description_of_parameters = {
        'name': 'Extract the full name of the applicant. Return as a single string. Avoid job titles or organization names.',
        'education': 'Capture degree or diploma name, field of study, institution, start_date, and end_date.',
        'experience': 'Each entry must include job title, company, start_date, end_date, and key responsibilities.',
        'skills': '''
        Carefully examine the "Skills" section of the resume. The content might appear as bullet points, paragraph text, comma-separated lists, or a combination of all.

        Extract all skill-related keywords and tool names from this section. Ignore section headers, filler words, and explanatory phrases.

        Return a flat list of distinct, meaningful skill keywords only — such as software tools, platforms, technologies, soft skills, or domain-specific skills.

        Do not return full sentences, descriptions, or repeated words. Just return clean, concise skill terms as a list.
        ''',
        'phone': 'Extract the applicant’s phone number if mentioned.',
        'address_location': 'Extract the address/location of the applicant only. Do not include job or institution locations.'
    }

    prompt = f"""
    You are a resume parser.

    Extract the following entities from the resume text delimited by triple backticks:
    ```{text}```

    Entities to extract: ```{json.dumps(listofparameters)}```

    Refer to this parameter description dictionary:
    ```{json.dumps(description_of_parameters, indent=2)}```

    Return a valid JSON object with these keys:
    name, education, experience, skills, phone, address_location

    Assign an empty list [] for any key with no value.
    Return only raw JSON. No explanation. Do not truncate your output.
    """

    llm_response = get_completion(prompt)
    match = re.search(r"```(?:json)?\s*(\{.*\})\s*```", llm_response, re.DOTALL)
    json_text = match.group(1) if match else llm_response.strip()

    try:
        return json.loads(json_text)
    except Exception as e:
        print("Failed to parse JSON from LLM:", e)
        return {
            'name': '',
            'education': [],
            'experience': [],
            'skills': [],
            'phone': '',
            'address_location': ''
        }

def get_titan_embedding(text):
    try:
        response = bedrock.invoke_model(
            body=json.dumps({"inputText": text}),
            modelId="amazon.titan-embed-text-v2:0",
            accept="application/json",
            contentType="application/json"
        )
        embedding = json.loads(response["body"].read())["embedding"]
        return embedding
    except Exception as e:
        print(f"ERROR: Titan embedding error: {e}") 
        raise 


def index_to_opensearch(doc_id, resume_text, metadata, embedding, bucket, filename):
    try:
        doc_body = {
            "id": doc_id,
            "resume_text": resume_text,
            "metadata": metadata, 
            "embedding": embedding,
            "s3_bucket": bucket,
            "s3_key": filename,
            "timestamp": int(time.time())
        }
        response = opensearch.index(index=INDEX_NAME, id=doc_id, body=doc_body)
        print(f"Document indexed: {response}")
    except Exception as e:
        print("OpenSearch indexing error:", e)

def upload_structured_info_to_dynamo(user_email, structured_info):
    try:
        item = {
            'userEmail': user_email,
            'name': structured_info.get('name', ''),
            'education': structured_info.get('education', []),
            'experience': structured_info.get('experience', []),
            'skills': structured_info.get('skills', []),
            'phone': structured_info.get('phone', ''),
            'address_location': structured_info.get('address_location', '')
        }

        table.put_item(Item=item)
        return {'statusCode': 200, 'message': f'Successfully uploaded resume data for {user_email}'}

    except Exception as e:
        print("Error uploading to DynamoDB:", e)
        return {'statusCode': 500, 'message': str(e)}


def lambda_handler(event, context):
    print("Received SQS Event:", json.dumps(event))

    for record in event.get('Records', []):
        receipt_handle = record['receiptHandle']
        error_occurred = False

        try:
            body = json.loads(record['body'])
            sns_message = json.loads(body['Message'])

            job_id = sns_message['JobId']
            job_status = sns_message['Status']
            if job_status != 'SUCCEEDED':
                raise Exception(f"Textract job did not succeed: {job_status}")

            bucket = sns_message['DocumentLocation']['S3Bucket']
            filename = sns_message['DocumentLocation']['S3ObjectName']
            userEmail = filename.split('/')[1]

            print(f"Processing file for user: {userEmail}")

            text_lines = get_textract_results(job_id)
            if not text_lines:
                raise Exception("Textract returned no text")

            resume_text = "\n".join(text_lines)
            print(resume_text)
            structured_info = get_entity(resume_text)
            print(f"structured_info: {structured_info}")
            upload_structured_info_to_dynamo(userEmail, structured_info)
            
            embedding = get_titan_embedding(resume_text)
            print(f"Embeddings: {embedding}")

            if not embedding:
                raise Exception("Titan embedding failed")
            # opensearch.delete(index=INDEX_NAME, id=userEmail, ignore=[404])
            index_to_opensearch(userEmail, resume_text, None, embedding, bucket, filename)
            print(f"SUCCESS: Resume indexed for user {userEmail}")

        except Exception as e:
            error_occurred = True
            print(f"ERROR: {str(e)}")

        finally:
            print("Deleting SQS message regardless of success or failure...")
            sqs.delete_message(QueueUrl=QUEUE_URL, ReceiptHandle=receipt_handle)

    return {
        'statusCode': 200,
        'body': json.dumps("Lambda execution completed.")
    }
